{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8dd58cd8",
      "metadata": {
        "id": "8dd58cd8"
      },
      "source": [
        "<!-- use this command in cmd - spark-shell -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d41a6356-fcde-4941-975a-3d1b45ec2cb4",
      "metadata": {
        "id": "d41a6356-fcde-4941-975a-3d1b45ec2cb4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAEbo0UEi9TM",
        "outputId": "8d362fbf-b350-451e-cb24-5cc2f47ef4c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=03d08406d975dd0f45612baccee5d08c01ed2173e4d3b239a43bffa50aa2c7a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: findspark, pyspark\n",
            "Successfully installed findspark-2.0.1 pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark findspark seaborn\n"
      ],
      "id": "IAEbo0UEi9TM"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cfb8a88d-ea5c-4df6-b7cb-9e7e72f83f3c",
      "metadata": {
        "id": "cfb8a88d-ea5c-4df6-b7cb-9e7e72f83f3c"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7679de48-9513-41e8-ab1a-01ffcb3752e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7679de48-9513-41e8-ab1a-01ffcb3752e0",
        "outputId": "2598f050-1bdb-4e9f-a999-d5f8f98b89c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+------+\n",
            "|row|col|result|\n",
            "+---+---+------+\n",
            "|  0|  0|    36|\n",
            "|  1|  1|    73|\n",
            "|  0|  1|    41|\n",
            "|  1|  0|    64|\n",
            "+---+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MatrixMultiplication\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample matrices\n",
        "matrix1 = [\n",
        "    (0, 0, 2),\n",
        "    (0, 1, 3),\n",
        "    (1, 0, 4),\n",
        "    (1, 1, 5)\n",
        "]\n",
        "\n",
        "matrix2 = [\n",
        "    (0, 0, 6),\n",
        "    (0, 1, 7),\n",
        "    (1, 0, 8),\n",
        "    (1, 1, 9)\n",
        "]\n",
        "\n",
        "# Create RDDs from the matrices\n",
        "matrix1_rdd = spark.sparkContext.parallelize(matrix1)\n",
        "matrix2_rdd = spark.sparkContext.parallelize(matrix2)\n",
        "\n",
        "# Perform matrix multiplication using map-reduce\n",
        "result_rdd = matrix1_rdd.flatMap(lambda x: [((x[0], y[1]), x[2] * y[2]) for y in matrix2 if x[1] == y[0]]). \\\n",
        "    reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "\n",
        "# Convert RDD to DataFrame\n",
        "result_df = spark.createDataFrame(result_rdd.map(lambda x: (x[0][0], x[0][1], x[1])), [\"row\", \"col\", \"result\"])\n",
        "\n",
        "# Display the result\n",
        "result_df.show()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rvp71b_-IRTp"
      },
      "id": "rvp71b_-IRTp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "44ae233e-0f7c-4884-9b69-55b37edd0ca3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44ae233e-0f7c-4884-9b69-55b37edd0ca3",
        "outputId": "8b91278f-b2c2-4988-9ac5-3a2bf7cd846f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+\n",
            "|Student|Subject|Grade|\n",
            "+-------+-------+-----+\n",
            "|  Alice|   Math|    A|\n",
            "|  Alice|Science|    A|\n",
            "|  Alice|English|    A|\n",
            "|    Bob|   Math|    B|\n",
            "|    Bob|Science|    B|\n",
            "|    Bob|English|    A|\n",
            "|Charlie|   Math|    B|\n",
            "|Charlie|Science|    B|\n",
            "|Charlie|English|    B|\n",
            "|  David|   Math|    A|\n",
            "|  David|Science|    A|\n",
            "|  David|English|    A|\n",
            "|    Eve|   Math|    B|\n",
            "|    Eve|Science|    A|\n",
            "|    Eve|English|    B|\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StudentGrades\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample student scores\n",
        "scores = [\n",
        "    (\"Alice\", {\"Math\": 85, \"Science\": 90, \"English\": 80}),\n",
        "    (\"Bob\", {\"Math\": 70, \"Science\": 75, \"English\": 85}),\n",
        "    (\"Charlie\", {\"Math\": 60, \"Science\": 65, \"English\": 70}),\n",
        "    (\"David\", {\"Math\": 90, \"Science\": 95, \"English\": 85}),\n",
        "    (\"Eve\", {\"Math\": 75, \"Science\": 80, \"English\": 75})\n",
        "]\n",
        "\n",
        "# Create RDD from the scores\n",
        "scores_rdd = spark.sparkContext.parallelize(scores)\n",
        "\n",
        "# Define the grading scheme (example)\n",
        "grading_scheme = {\n",
        "    \"A\": (80, 100),\n",
        "    \"B\": (60, 79),\n",
        "    \"C\": (40, 59),\n",
        "    \"D\": (0, 39)\n",
        "}\n",
        "\n",
        "# Function to compute grades for a given score\n",
        "def compute_grade(score):\n",
        "    for grade, (lower_bound, upper_bound) in grading_scheme.items():\n",
        "        if lower_bound <= score <= upper_bound:\n",
        "            return grade\n",
        "    return \"F\"\n",
        "\n",
        "# Map operation to compute grades for each student\n",
        "grades_rdd = scores_rdd.map(lambda x: (x[0], {subject: compute_grade(score) for subject, score in x[1].items()}))\n",
        "\n",
        "# Convert RDD to DataFrame\n",
        "grades_df = spark.createDataFrame(grades_rdd.flatMap(lambda x: [(x[0], subject, grade) for subject, grade in x[1].items()]), [\"Student\", \"Subject\", \"Grade\"])\n",
        "\n",
        "# Display the result\n",
        "grades_df.show()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8dce8afe-3718-4630-949d-5d308e8fd6ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dce8afe-3718-4630-949d-5d308e8fd6ed",
        "outputId": "a31998e7-3c55-45f4-b86f-6e71f1a32ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of male passengers who died: 468\n",
            "Average age of male passengers who died: 24.321581196581196\n",
            "Number of deceased passengers in each class among females:\n",
            "+------+-----+\n",
            "|Pclass|count|\n",
            "+------+-----+\n",
            "|     1|    3|\n",
            "|     3|   72|\n",
            "|     2|    6|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TitanicAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the Titanic dataset\n",
        "titanic_df = spark.createDataFrame(sns.load_dataset(\"titanic\"))\n",
        "\n",
        "titanic_df = titanic_df.fillna({'Age': 0})\n",
        "\n",
        "# Filter data for male passengers who died and remove null values from Age column\n",
        "male_deceased = titanic_df.filter((titanic_df[\"Sex\"] == \"male\") & (titanic_df[\"Survived\"] == 0) & titanic_df[\"Age\"].isNotNull())\n",
        "\n",
        "# Check if there are any male passengers who died\n",
        "male_deceased_count = male_deceased.count()\n",
        "\n",
        "if male_deceased_count > 0:\n",
        "    # Calculate the average age of male passengers who died\n",
        "# Calculate the average age of male passengers who died\n",
        "    male_deceased_age_avg = male_deceased.agg({\"Age\": \"avg\"}).collect()[0][0]\n",
        "    print(\"Number of male passengers who died:\", male_deceased_count)\n",
        "    print(\"Average age of male passengers who died:\", male_deceased_age_avg)\n",
        "else:\n",
        "    print(\"No male passengers found who died in the dataset.\")\n",
        "\n",
        "female_deceased_by_class = titanic_df.filter((titanic_df[\"Sex\"] == \"female\") & (titanic_df[\"Survived\"] == 0)).groupBy(\"Pclass\").count()\n",
        "\n",
        "# Display results\n",
        "print(\"Number of deceased passengers in each class among females:\")\n",
        "female_deceased_by_class.show()\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c6b3f7c-99f7-414f-a5be-2aef637eff89",
      "metadata": {
        "id": "2c6b3f7c-99f7-414f-a5be-2aef637eff89"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}